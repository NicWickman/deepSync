{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio.datasets.utils import download_url, extract_archive, walk_files\n",
    "\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "import fastai\n",
    "import fastai.vision\n",
    "from fastai.basics import *\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path = Path('D:/MachineLearning/deepSync/data/')\n",
    "\n",
    "class AirbudDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Create a Dataset for Audio. Each item is a tuple of the form:\n",
    "    (waveform, sample_rate, labels)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        data_path='data',\n",
    "        anim_csv='data2.csv',\n",
    "        audio_ext='.aif',\n",
    "        download=False,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    ):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # archive = os.path.basename(url)\n",
    "        # archive = os.path.join(root, archive)\n",
    "        # self._path = os.path.join(root, folder_in_archive)\n",
    "\n",
    "        # if download:\n",
    "        #     if not os.path.isdir(self._path):\n",
    "        #         if not os.path.isfile(archive):\n",
    "        #             download_url(url, root)\n",
    "        #         extract_archive(archive)\n",
    "\n",
    "        self._audio_path = root/data_path/Path('Audio')\n",
    "        self._anim_path = root/data_path/Path(anim_csv)\n",
    "        self._ext_audio = audio_ext\n",
    "\n",
    "        if not os.path.isdir(self._audio_path):\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found. Please use `download=True` to download it.\"\n",
    "            )\n",
    "\n",
    "        walker = walk_files(\n",
    "            self._audio_path, suffix=self._ext_audio, prefix=False, remove_suffix=True\n",
    "        )\n",
    "\n",
    "        self._walker = list(walker)\n",
    "        self.df = self._process_df(pd.read_csv(self._anim_path))\n",
    "        \n",
    "\n",
    "    def _process_df(self, df):\n",
    "        def convert_to_list(x):\n",
    "          try:\n",
    "              y = literal_eval(x)\n",
    "              return y\n",
    "          except:\n",
    "              return x\n",
    "        \n",
    "        def convert_floats(x):\n",
    "            if isinstance(x, float):\n",
    "                return [x]\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "        df['jawTrans_ty'] = df['jawTrans_ty'].fillna('0.0')\n",
    "        df['jawTrans_ty'] = df['jawTrans_ty'].apply(lambda x: convert_to_list(x))\n",
    "        df['jawTrans_ty'] = df['jawTrans_ty'].apply(lambda x: convert_floats(x))\n",
    "        return df\n",
    "\n",
    "    def _get_labels(self, fileid):\n",
    "        y_vals = self.df[self.df['Audio File'].str.contains(fileid)]['jawTrans_ty'].values[0]\n",
    "        return y_vals\n",
    "\n",
    "\n",
    "    def _load_audio_item(self, fileid):\n",
    "        # Read label\n",
    "        labels = self._get_labels(fileid)\n",
    "\n",
    "        # Read audio\n",
    "        filepath = str(self._audio_path / Path(fileid)) + self._ext_audio\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(filepath)\n",
    "\n",
    "        return waveform, sample_rate, labels\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        fileid = self._walker[n]\n",
    "        item = self._load_audio_item(fileid)\n",
    "\n",
    "        waveform, sample_rate, labels = item\n",
    "        \n",
    "        labels = torch.Tensor(labels).unsqueeze(dim=0)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            waveform = self.transform(waveform)\n",
    "        if self.target_transform is not None:\n",
    "            labels = self.target_transform(labels)\n",
    "\n",
    "        return waveform, sample_rate, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._walker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadTrim(object):\n",
    "    \"\"\"Pad/Trim a 1d-Tensor (Signal or Labels)\n",
    "    Args:\n",
    "        tensor (Tensor): Tensor of audio of size (n x c) or (c x n)\n",
    "        max_len (int): Length to which the tensor will be padded\n",
    "        channels_first (bool): Pad for channels first tensors.  Default: `True`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_len, fill_value=0, channels_first=True):\n",
    "        self.max_len = max_len\n",
    "        self.fill_value = fill_value\n",
    "        self.len_dim, self.ch_dim = int(channels_first), int(not channels_first)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Tensor: (c x n) or (n x c)\n",
    "        \"\"\"\n",
    "        assert tensor.size(self.ch_dim) < 128, \\\n",
    "            \"Too many channels ({}) detected, see channels_first param.\".format(tensor.size(self.ch_dim))\n",
    "        if self.max_len > tensor.size(self.len_dim):\n",
    "            padding = [self.max_len - tensor.size(self.len_dim)\n",
    "                       if (i % 2 == 1) and (i // 2 != self.len_dim)\n",
    "                       else 0\n",
    "                       for i in range(4)]\n",
    "            with torch.no_grad():\n",
    "                tensor = torch.nn.functional.pad(tensor, padding, \"constant\", self.fill_value)\n",
    "        elif self.max_len < tensor.size(self.len_dim):\n",
    "            tensor = tensor.narrow(self.len_dim, 0, self.max_len)\n",
    "        return tensor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(max_len={0})'.format(self.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch \n",
    "transform = transforms.Compose([\n",
    "#                                 torchaudio.transforms.AmplitudeToDB(stype='power', top_db=None),\n",
    "#                                 PadTrim(500),\n",
    "                                torchaudio.transforms.Spectrogram(n_fft=400, win_length=None, hop_length=None, pad=0, power=2, normalized=False),\n",
    "                                transforms.ToPILImage(),\n",
    "#                                 transforms.Resize((240,240)),\n",
    "                                transforms.ToTensor()]\n",
    "                               )\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "                                PadTrim(240)\n",
    "                                ]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('D:\\MachineLearning\\deepSync')\n",
    "dataset = AirbudDataset(root, transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201, 831])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d556cfac88>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset[2][0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "def show_audio(ad):\n",
    "    sig,sr=ad\n",
    "    display(Audio(data=sig, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Array audio input must be a 1D or 2D array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-4a77275e9051>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshow_audio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-139-a83ccd92d334>\u001b[0m in \u001b[0;36mshow_audio\u001b[1;34m(ad)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow_audio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAudio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\fastaiv1\\lib\\site-packages\\IPython\\lib\\display.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rate must be specified when data is a numpy array or list of audio samples.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_wav\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\fastaiv1\\lib\\site-packages\\IPython\\lib\\display.py\u001b[0m in \u001b[0;36m_make_wav\u001b[1;34m(data, rate, normalize)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mscaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnchan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_and_normalize_with_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mscaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnchan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_and_normalize_without_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\fastaiv1\\lib\\site-packages\\IPython\\lib\\display.py\u001b[0m in \u001b[0;36m_validate_and_normalize_with_numpy\u001b[1;34m(data, normalize)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Array audio input must be a 1D or 2D array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mmax_abs_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Array audio input must be a 1D or 2D array"
     ]
    }
   ],
   "source": [
    "show_audio((dataset[0][0], dataset[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastaiv1] *",
   "language": "python",
   "name": "conda-env-fastaiv1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
